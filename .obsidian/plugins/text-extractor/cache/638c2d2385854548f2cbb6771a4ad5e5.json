{"path":".obsidian/plugins/text-extractor/cache/638c2d2385854548f2cbb6771a4ad5e5.json","text":"Intro to Database Systems 15-445/15-645 Fall 2019 Andy Pavlo Computer Science Carnegie Mellon UniversityAP 26 Final Review + Systems Potpourri CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Project #4: Tuesday Dec 10th @ 11:59pm Extra Credit: Tuesday Dec 10th @ 11:59pm Final Exam: Monday Dec 9th @ 5:30pm 2 CMU 15-445/645 (Fall 2019) F I N A L E X A M Who: You What: http://cmudb.io/f19-final When: Monday Dec 9th @ 5:30pm Where: Porter Hall 100 Why: https://youtu.be/6yOH_FjeSAQ 3 CMU 15-445/645 (Fall 2019) F I N A L E X A M What to bring: → CMU ID → One page of handwritten notes (double-sided) → Extra Credit Coupon Optional: → Spare change of clothes → Food What not to bring: → Your roommate 4 CMU 15-445/645 (Fall 2019) C O U R S E E VA L S Your feedback is strongly needed: → https://cmu.smartevals.com Things that we want feedback on: → Homework Assignments → Projects → Reading Materials → Lectures 5 CMU 15-445/645 (Fall 2019) O F F I C E H O U R S Andy's hours: → Friday Dec 6th @ 3:30-4:30pm → Monday Dec 9th @ 1:30-2:30pm All TAs will have their regular office hours up to and including Saturday Dec 14th 6 CMU 15-445/645 (Fall 2019) S T U F F B E F O R E M I D -TERM SQL Buffer Pool Management Hash Tables B+Trees Storage Models Inter-Query Parallelism 7 CMU 15-445/645 (Fall 2019) T R A N S A C T I O N S ACID Conflict Serializability: → How to check? → How to ensure? View Serializability Recoverable Schedules Isolation Levels / Anomalies 9 CMU 15-445/645 (Fall 2019) T R A N S A C T I O N S Two-Phase Locking → Rigorous vs. Non-Rigorous → Deadlock Detection & Prevention Multiple Granularity Locking → Intention Locks 10 CMU 15-445/645 (Fall 2019) T R A N S A C T I O N S Timestamp Ordering Concurrency Control → Thomas Write Rule Optimistic Concurrency Control → Read Phase → Validation Phase → Write Phase Multi-Version Concurrency Control → Version Storage / Ordering → Garbage Collection 11 CMU 15-445/645 (Fall 2019) C R A S H R E C O V E R Y Buffer Pool Policies: → STEAL vs. NO-STEAL → FORCE vs. NO-FORCE Write-Ahead Logging Logging Schemes Checkpoints ARIES Recovery → Log Sequence Numbers → CLRs 12 CMU 15-445/645 (Fall 2019) D I S T R I B U T E D D ATA B A S E S System Architectures Replication Partitioning Schemes Two-Phase Commit 13 CMU 15-445/645 (Fall 2019) 26 25 24 18 18 17 12 11 10 10 152018 20 19 18 17 17 17 17 16 15 15 2019 CMU 15-445/645 (Fall 2019) 16 CMU 15-445/645 (Fall 2019) FA C E B O O K S C U B A Internal DBMS designed for real-time data analysis of performance monitoring data. → Columnar Storage Model → Distributed / Shared-Nothing → Tiered-Storage → No Joins or Global Sorting → Heterogeneous Hierarchical Distributed Architecture Designed for low-latency ingestion and queries. Redundant deployments with lossy fault-tolerance. 17 CMU 15-445/645 (Fall 2019) FA C E B O O K LO G P I P E L I N E 18 Record Batcher Record Batcher Record Batcher…Combined Logs Per Category Streaming Platform Scuba Structured Debug Logs… Columnar Batch Data Execution Layer SQL Queries Aggregator Aggregator Aggregator Application Servers Leaf Node Leaf Node Leaf NodeValidation Service Insert Counters Source: Stavros Harizopoulos CMU 15-445/645 (Fall 2019) S C U B A A R C H I T E C T U R E Leaf Nodes: → Store columnar data on local SSDs. → Leaf nodes may or may not contain data needed for a query. → No indexes. All scanning is done on time ranges. Aggregator Nodes: → Dispatch plan fragments to all its children leaf nodes. → Combine the results from children. → If a leaf node does not produce results before a timeout, then they are omitted. 19 CMU 15-445/645 (Fall 2019) S C U B A A R C H I T E C T U R E 20 … … SELECT COUNT(*) FROM events WHERE type = 'crash' AND time = 'Monday' Root Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Aggregator Aggregator Aggregator Query Plan Fragments CMU 15-445/645 (Fall 2019) S C U B A A R C H I T E C T U R E 20 … … SELECT COUNT(*) FROM events WHERE type = 'crash' AND time = 'Monday' Root Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Aggregator Aggregator Aggregator Query Plan Fragments CMU 15-445/645 (Fall 2019) S C U B A A R C H I T E C T U R E 20 … … SELECT COUNT(*) FROM events WHERE type = 'crash' AND time = 'Monday' 10 20 25 15 20 Root Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Aggregator Aggregator Aggregator Query Plan Fragments CMU 15-445/645 (Fall 2019) S C U B A A R C H I T E C T U R E 20 … … SELECT COUNT(*) FROM events WHERE type = 'crash' AND time = 'Monday' 10 20 25 15 20 30+40+20=90 Root 10+20=30 25+15=40 20 Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Leaf Node Aggregator Aggregator Aggregator CMU 15-445/645 (Fall 2019) FA U LT T O L E R A N C E Facebook maintains multiple Scuba clusters that contain the same databases. Every query is executed on all the clusters at the same time. It compares the amount of missing data each cluster had when executing the query to determine which one produced the most accurate result. → Track the number of tuples examined vs. number of tuples inserted via Validation Service. 21 CMU 15-445/645 (Fall 2019) 22 CMU 15-445/645 (Fall 2019) M O N G O D B Distributed document DBMS started in 2007. → Document → Tuple → Collection → Table/Relation Open-source (Server Side Public License) Centralized shared-nothing architecture. Concurrency Control: → OCC with multi-granular locking 23 CMU 15-445/645 (Fall 2019) P H Y S I C A L D E N O R M A L I Z AT I O N A customer has orders and each order has order items. 24 Customers Orders Order Items R2(orderId,custId,…) R1(custId,name,…) R3(itemId,orderId,…) ⨝ ⨝ CMU 15-445/645 (Fall 2019) P H Y S I C A L D E N O R M A L I Z AT I O N A customer has orders and each order has order items. 24 Customers Orders Order Items Customer OrdersOrdersOrder Order Item Order Item ⋮ CMU 15-445/645 (Fall 2019) P H Y S I C A L D E N O R M A L I Z AT I O N A customer has orders and each order has order items. 24 Customers Orders Order Items { \"custId\": 1234, \"custName\": \"Andy\", \"orders\": [ { \"orderId\": 9999, \"orderItems\": [ { \"itemId\": \"XXXX\", \"price\": 19.99 }, { \"itemId\": \"YYYY\", \"price\": 29.99 }, ] } ] } CMU 15-445/645 (Fall 2019) Q U E R Y E X E C U T I O N JSON-only query API No cost-based query planner / optimizer. → Heuristic-based + \"random walk\" optimization. JavaScript UDFs (not encouraged). Supports server-side joins (only left-outer?). Multi-document transactions. 25 CMU 15-445/645 (Fall 2019) D I S T R I B U T E D A R C H I T E C T U R E Heterogeneous distributed components. → Shared nothing architecture → Centralized query router. Master-slave replication. Auto-sharding: → Define 'partitioning' attributes for each collection (hash or range). → When a shard gets too big, the DBMS automatically splits the shard and rebalances. 26 CMU 15-445/645 (Fall 2019) M O N G O D B C L U S T E R A R C H I T E C T U R E 27 Router (mongos) Shards (mongod) P3 P4 P1 P2 P1→ID:1-100 P2→ID:101-200 P3→ID:201-300 P4→ID:301-400 Config Server (mongod) Router (mongos) ⋮ ⋮ Application Server Get Id=101 CMU 15-445/645 (Fall 2019) S T O R A G E A R C H I T E C T U R E Originally used mmap storage manager → No buffer pool. → Let the OS decide when to flush pages. → Single lock per database. MongoDB v3 supports pluggable storage backends → WiredTiger from BerkeleyDB alumni. http://cmudb.io/lectures2015-wiredtiger → RocksDB from Facebook (“MongoRocks”) http://cmudb.io/lectures2015-rocksdb 28 CMU 15-445/645 (Fall 2019) 29 CMU 15-445/645 (Fall 2019) C O C K R O A C H D B Started in 2015 by ex-Google employees. Open-source (BSL – MariaDB) Decentralized shared-nothing architecture using range partitioning. Log-structured on-disk storage (RocksDB) Concurrency Control: → MVCC + OCC → Serializable isolation only 30 CMU 15-445/645 (Fall 2019) D I S T R I B U T E D A R C H I T E C T U R E Multi-layer architecture on top of a replicated key-value store. → All tables and indexes are store in a giant sorted map in the k/v store. Uses RocksDB as the storage manager at each node. Raft protocol (variant of Paxos) for replication and consensus. 31 SQL Layer Transactional Key-Value Router Replication Storage CMU 15-445/645 (Fall 2019) C O N C U R R E N C Y C O N T R O L DBMS uses hybrid clocks (physical + logical) to order transactions globally. → Synchronized wall clock with local counter. Txns stage writes as \"intents\" and then checks for conflicts on commit. All meta-data about txns state resides in the key- value store. 32 CMU 15-445/645 (Fall 2019) C O C K R O A C H D B O V E R V I E W 33 Node 1 Node 2 Node 3 Application ID:1-100 →Node1 ID:101-200 →Node2 ID:201-300 →Node3 1-100 101-200 201-300 1-100 101-200 201-300 1-100 101-200 201-300 … Node n Id=50 CMU 15-445/645 (Fall 2019) C O C K R O A C H D B O V E R V I E W 33 Node 1 Node 2 Node 3 Leader Application Raft Raft ID:1-100 →Node1 ID:101-200 →Node2 ID:201-300 →Node3 1-100 101-200 201-300 1-100 101-200 201-300 1-100 101-200 201-300 Update Id=50 … Node n CMU 15-445/645 (Fall 2019) C O C K R O A C H D B O V E R V I E W 33 Node 1 Node 2 Node 3 Application ID:1-100 →Node1 ID:101-200 →Node2 ID:201-300 →Node3 1-100 101-200 201-300 1-100 101-200 201-300 1-100 101-200 201-300 … Node n Id=150 CMU 15-445/645 (Fall 2019) C O C K R O A C H D B O V E R V I E W 33 Node 1 Node 2 Node 3 Application ID:1-100 →Node1 ID:101-200 →Node2 ID:201-300 →Node3 1-100 101-200 201-300 1-100 101-200 201-300 1-100 101-200 201-300 Get Id=150 … Node n Leader CMU 15-445/645 (Fall 2019) A N DY ' S C O N C L U D I N G R E M A R K S Databases are awesome. → They cover all facets of computer science. → We have barely scratched the surface… Going forth, you should now have a good understanding how these systems work. This will allow you to make informed decisions throughout your entire career. → Avoid premature optimizations. 34","libVersion":"0.2.2","langs":""}