{"path":".obsidian/plugins/text-extractor/cache/1d153b72c640232a02b6eaa5c49d0739.json","text":"Intro to Database Systems 15-445/15-645 Fall 2019 Andy Pavlo Computer Science Carnegie Mellon UniversityAP 09 Index Concurrency CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Project #1 is due Fri Sept 27th @ 11:59pm Homework #2 is due Mon Sept 30th @ 11:59pm Project #2 will be released Mon Sept 30th 2 CMU 15-445/645 (Fall 2019) O B S E R VAT I O N We assumed that all the data structures that we have discussed so far are single-threaded. But we need to allow multiple threads to safely access our data structures to take advantage of additional CPU cores and hide disk I/O stalls. 3 They Don't Do This! CMU 15-445/645 (Fall 2019) C O N C U R R E N C Y C O N T R O L A concurrency control protocol is the method that the DBMS uses to ensure \"correct\" results for concurrent operations on a shared object. A protocol's correctness criteria can vary: → Logical Correctness: Can I see the data that I am supposed to see? → Physical Correctness: Is the internal representation of the object sound? 4 CMU 15-445/645 (Fall 2019) T O D AY ' S A G E N D A Latches Overview Hash Table Latching B+Tree Latching Leaf Node Scans Delayed Parent Updates 5 CMU 15-445/645 (Fall 2019) LO C K S V S . L AT C H E S Locks → Protects the database's logical contents from other txns. → Held for txn duration. → Need to be able to rollback changes. Latches → Protects the critical sections of the DBMS's internal data structure from other threads. → Held for operation duration. → Do not need to be able to rollback changes. 6 CMU 15-445/645 (Fall 2019) LO C K S V S . L AT C H E S 7 Locks Latches Separate… User transactions Threads Protect… Database Contents In-Memory Data Structures During… Entire Transactions Critical Sections Modes… Shared, Exclusive, Update, Intention Read, Write Deadlock Detection & Resolution Avoidance …by… Waits-for, Timeout, Aborts Coding Discipline Kept in… Lock Manager Protected Data Structure Source: Goetz Graefe CMU 15-445/645 (Fall 2019) L AT C H M O D E S Read Mode → Multiple threads can read the same object at the same time. → A thread can acquire the read latch if another thread has it in read mode. Write Mode → Only one thread can access the object. → A thread cannot acquire a write latch if another thread holds the latch in any mode. 8 Read Write Read ✔ X Write X X Compatibility Matrix CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #1: Blocking OS Mutex → Simple to use → Non-scalable (about 25ns per lock/unlock invocation) → Example: std::mutex 9 std::mutex m; ⋮ m.lock(); // Do something special... m.unlock(); CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #2: Test-and-Set Spin Latch (TAS) → Very efficient (single instruction to latch/unlatch) → Non-scalable, not cache friendly → Example: std::atomic<T> 10 std::atomic_flag latch; ⋮ while (latch.test_and_set(…)) { // Retry? Yield? Abort? } CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #3: Reader-Writer Latch → Allows for concurrent readers → Must manage read/write queues to avoid starvation → Can be implemented on top of spinlocks 11 read write Latch =0 =0 =0 =0 CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #3: Reader-Writer Latch → Allows for concurrent readers → Must manage read/write queues to avoid starvation → Can be implemented on top of spinlocks 11 read write Latch =0 =0 =0 =0 =1 CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #3: Reader-Writer Latch → Allows for concurrent readers → Must manage read/write queues to avoid starvation → Can be implemented on top of spinlocks 11 read write Latch =0 =0 =0 =0 =1=2 CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #3: Reader-Writer Latch → Allows for concurrent readers → Must manage read/write queues to avoid starvation → Can be implemented on top of spinlocks 11 read write Latch =0 =0 =0 =0 =1=2 =1 CMU 15-445/645 (Fall 2019) L AT C H I M P L E M E N TAT I O N S Approach #3: Reader-Writer Latch → Allows for concurrent readers → Must manage read/write queues to avoid starvation → Can be implemented on top of spinlocks 11 read write Latch =0 =0 =0 =0 =1=2 =1=1 CMU 15-445/645 (Fall 2019) H A S H TA B L E L AT C H I N G Easy to support concurrent access due to the limited ways threads access the data structure. → All threads move in the same direction and only access a single page/slot at a time. → Deadlocks are not possible. To resize the table, take a global latch on the entire table (i.e., in the header page). 12 CMU 15-445/645 (Fall 2019) H A S H TA B L E L AT C H I N G Approach #1: Page Latches → Each page has its own reader-write latch that protects its entire contents. → Threads acquire either a read or write latch before they access a page. Approach #2: Slot Latches → Each slot has its own latch. → Can use a single mode latch to reduce meta-data and computational overhead. 13 CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB hash(D) T1: Find D CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB R hash(D) T1: Find D CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB R hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB R hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB R hash(D) T1: Find D hash(E) T2: Insert E 0 1 2 It’s safe to release the latch on Page #1. CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB hash(D) T1: Find D R hash(E) T2: Insert E 0 1 2 CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB hash(D) T1: Find D R hash(E) T2: Insert E W 0 1 2 CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB hash(D) T1: Find D hash(E) T2: Insert E W 0 1 2 CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB hash(D) T1: Find D hash(E) T2: Insert E 0 1 2 W CMU 15-445/645 (Fall 2019) | valD | valE | valA | valC H A S H TA B L E PA G E L AT C H E S 14 | valB hash(D) T1: Find D hash(E) T2: Insert E 0 1 2 W CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB 0 1 2 hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB R 0 1 2 hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB R 0 1 2 hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB R 0 1 2 W hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB R 0 1 2 W hash(D) T1: Find D hash(E) T2: Insert E It’s safe to release the latch on A CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB 0 1 2 W hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB 0 1 2 W hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB 0 1 2 W R hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valE | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB 0 1 2 R W hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) | valD | valE | valA | valC H A S H TA B L E S LO T L AT C H E S 15 | valB R 0 1 2W hash(D) T1: Find D hash(E) T2: Insert E CMU 15-445/645 (Fall 2019) B + T R E E C O N C U R R E N C Y C O N T R O L We want to allow multiple threads to read and update a B+Tree at the same time. We need to protect from two types of problems: → Threads trying to modify the contents of a node at the same time. → One thread traversing the tree while another thread splits/merges nodes. 16 CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 41 CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 41 CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 41 CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 41 Rebalance! CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 T2: Find 41 41 Rebalance! CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 T2: Find 41 41 Rebalance! CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 T2: Find 41 41 Rebalance! CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 T2: Find 41 41 Rebalance! 41 CMU 15-445/645 (Fall 2019) 38 B + T R E E M U LT I -T H R E A D E D E X A M P L E 17 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 T1: Delete 44 T2: Find 41 41 Rebalance! 41 ??? CMU 15-445/645 (Fall 2019) L AT C H C R A B B I N G / C O U P L I N G Protocol to allow multiple threads to access/modify B+Tree at the same time. Basic Idea: → Get latch for parent. → Get latch for child → Release latch for parent if “safe”. A safe node is one that will not split or merge when updated. → Not full (on insertion) → More than half-full (on deletion) 18 CMU 15-445/645 (Fall 2019) L AT C H C R A B B I N G / C O U P L I N G Find: Start at root and go down; repeatedly, → Acquire R latch on child → Then unlatch parent Insert/Delete: Start at root and go down, obtaining W latches as needed. Once child is latched, check if it is safe: → If child is safe, release all latches on ancestors. 19 CMU 15-445/645 (Fall 2019) E X A M P L E # 1 F I N D 38 20 3 4 6 9 10 11 12 13 20 22 23 31 35 36 38 41 44 20 6 12 23 38 44 B C D E F G H I 3510 R A CMU 15-445/645 (Fall 2019) E X A M P L E # 1 F I N D 38 20 3 4 6 9 10 11 12 13 20 22 23 31 35 36 38 41 44 20 6 12 23 38 44 B C D E F G H I 3510 R R It’s safe to release the latch on A. A CMU 15-445/645 (Fall 2019) E X A M P L E # 1 F I N D 38 20 3 4 6 9 10 11 12 13 20 22 23 31 35 36 38 41 44 20 6 12 23 38 44 B C D E F G H I 3510 R A CMU 15-445/645 (Fall 2019) E X A M P L E # 1 F I N D 38 20 3 4 6 9 10 11 12 13 20 22 23 31 35 36 38 41 44 20 6 12 23 38 44 B C D E F G H I 3510 R A CMU 15-445/645 (Fall 2019) E X A M P L E # 1 F I N D 38 20 3 4 6 9 10 11 12 13 20 22 23 31 35 36 38 41 44 20 6 12 23 38 44 B C D E F G H I 3510 R A CMU 15-445/645 (Fall 2019) E X A M P L E # 1 F I N D 38 20 3 4 6 9 10 11 12 13 20 22 23 31 35 36 38 41 44 20 6 12 23 38 44 B C D E F G H I 3510 A CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 21 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 21 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W W We may need to coalesce B, so we can’t release the latch on A. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 21 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W W W We know that D will not need to merge with C, so it’s safe to release latches on A and B. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 21 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W We know that D will not need to merge with C, so it’s safe to release latches on A and B. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 21 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 21 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 3 I N S E R T 4 5 22 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 45 20 6 12 23 38 44 A B C D E F G H I 3510 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 3 I N S E R T 4 5 22 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 45 20 6 12 23 38 44 A B C D E F G H I 3510 W W We know that if D needs to split, B has room so it’s safe to release the latch on A. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 3 I N S E R T 4 5 22 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 45 20 6 12 23 38 44 A B C D E F G H I 3510 W W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 3 I N S E R T 4 5 22 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 45 20 6 12 23 38 44 A B C D E F G H I 3510 W W W Node I won’t split, so we can release B+D. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 3 I N S E R T 4 5 22 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 45 20 6 12 23 38 44 A B C D E F G H I 3510 W Node I won’t split, so we can release B+D. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W W We need to split F so we need to hold the latch on its parent node. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W W 25 We need to split F so we need to hold the latch on its parent node. CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 23 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W W 25 31 We need to split F so we need to hold the latch on its parent node. CMU 15-445/645 (Fall 2019) O B S E R VAT I O N What was the first step that all the update examples did on the B+Tree? 24 20 A W Delete 38 20 A W Insert 45 20 A W Insert 25 CMU 15-445/645 (Fall 2019) O B S E R VAT I O N What was the first step that all the update examples did on the B+Tree? Taking a write latch on the root every time becomes a bottleneck with higher concurrency. Can we do better? 25 CMU 15-445/645 (Fall 2019) B E T T E R L AT C H I N G A LG O R I T H M Assume that the leaf node is safe. Use read latches and crabbing to reach it, and then verify that it is safe. If leaf is not safe, then do previous algorithm using write latches. 26 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 27 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 R CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 27 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 R CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 27 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 R CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 27 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 R W CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 27 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W H will not need to coalesce, so we’re safe! CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 2 D E L E T E 3 8 27 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W H will not need to coalesce, so we’re safe! CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 28 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 38 44 A B C D E F G H I 3510 W We need to split F so we have to restart and re- execute like before. CMU 15-445/645 (Fall 2019) B E T T E R L AT C H I N G A LG O R I T H M Search: Same as before. Insert/Delete: → Set latches as if for search, get to leaf, and set W latch on leaf. → If leaf is not safe, release all latches, and restart thread using previous insert/delete protocol with write latches. This approach optimistically assumes that only leaf node will be modified; if not, R latches set on the first pass to leaf are wasteful. 29 CMU 15-445/645 (Fall 2019) O B S E R VAT I O N The threads in all the examples so far have acquired latches in a \"top-down\" manner. → A thread can only acquire a latch from a node that is below its current node. → If the desired latch is unavailable, the thread must wait until it becomes available. But what if we want to move from one leaf node to another leaf node? 30 CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 1 31 A B 3 1 2 3 4 C T1: Find Keys < 4 R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 1 31 A B 3 1 2 3 4 C T1: Find Keys < 4 R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 1 31 A B 3 1 2 3 4 C T1: Find Keys < 4 R Do not release latch on C until thread has latch on B CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 1 31 A B 3 1 2 3 4 C T1: Find Keys < 4 R R Do not release latch on C until thread has latch on B CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 1 31 A B 3 1 2 3 4 C T1: Find Keys < 4 R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 2 32 A B 3 1 2 3 4 C T1: Find Keys < 4 T2: Find Keys > 1 CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 2 32 A B 3 1 2 3 4 C T1: Find Keys < 4 T2: Find Keys > 1 R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 2 32 A B 3 1 2 3 4 C T1: Find Keys < 4 T2: Find Keys > 1 R R R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 2 32 A B 3 1 2 3 4 C T1: Find Keys < 4 T2: Find Keys > 1 R R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 2 32 A B 3 1 2 3 4 C T1: Find Keys < 4 T2: Find Keys > 1 R R Both T1 and T2 now hold this read latch. Both T1 and T2 now hold this read latch. CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 2 32 A B 3 1 2 3 4 C T1: Find Keys < 4 T2: Find Keys > 1 R R Only T1 holds this read latch. Only T2 holds this read latch. CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 3 33 A B 3 1 2 3 4 C T1: Delete 4 T2: Find Keys > 1 R CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 3 33 A B 3 1 2 3 4 C T1: Delete 4 T2: Find Keys > 1 R W CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 3 33 A B 3 1 2 3 4 C T1: Delete 4 T2: Find Keys > 1 R W T2 cannot acquire the read latch on C CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 3 33 A B 3 1 2 3 4 C T1: Delete 4 T2: Find Keys > 1 R W T2 does not know what T1 is doing… T2 cannot acquire the read latch on C CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N E X A M P L E # 3 33 A B 3 1 2 3 4 C T1: Delete 4 T2: Find Keys > 1 R W T2 does not know what T1 is doing… T2 cannot acquire the read latch on C CMU 15-445/645 (Fall 2019) L E A F N O D E S C A N S Latches do not support deadlock detection or avoidance. The only way we can deal with this problem is through coding discipline. The leaf node sibling latch acquisition protocol must support a \"no-wait\" mode. The DBMS's data structures must cope with failed latch acquisitions. 34 CMU 15-445/645 (Fall 2019) D E L AY E D PA R E N T U P D AT E S Every time a leaf node overflows, we must update at least three nodes. → The leaf node being split. → The new leaf node being created. → The parent node. Blink-Tree Optimization: When a leaf node overflows, delay updating its parent node. 35 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R R T1: Insert 25 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R R T1: Insert 25 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R T1: Insert 25 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W Add the new leaf node as a sibling to F, but do not update C T1: Insert 25 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 W 25 31 Add the new leaf node as a sibling to F, but do not update C T1: Insert 25 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 25 31 Add the new leaf node as a sibling to F, but do not update C T1: Insert 25 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 25 31 Update C the next time that a thread takes a write latch on it. T1: Insert 25 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 25 31 T1: Insert 25 T2: Find 31 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 25 31 T1: Insert 25 T2: Find 31 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R 25 31 T1: Insert 25 T2: Find 31 T3: Insert 33 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R 25 31 T1: Insert 25 T2: Find 31 T3: Insert 33 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R 25 31 W T1: Insert 25 T2: Find 31 T3: Insert 33 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R 25 31 W T1: Insert 25 T2: Find 31 T3: Insert 33 C: Add 31 CMU 15-445/645 (Fall 2019) 38 41 E X A M P L E # 4 I N S E R T 2 5 36 3 4 6 9 10 11 12 13 20 22 23 31 35 36 44 20 6 12 23 31 38 44 A B C D E F G H I 3510 R 25 31 W T1: Insert 25 T2: Find 31 T3: Insert 33 33 C: Add 31 W CMU 15-445/645 (Fall 2019) C O N C L U S I O N Making a data structure thread-safe is notoriously difficult in practice. We focused on B+Trees but the same high-level techniques are applicable to other data structures. 37 CMU 15-445/645 (Fall 2019) N E X T C L A S S We are finally going to discuss how to execute some queries… 38","libVersion":"0.2.2","langs":""}