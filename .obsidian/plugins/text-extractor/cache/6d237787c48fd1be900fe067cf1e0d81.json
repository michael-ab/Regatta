{"path":".obsidian/plugins/text-extractor/cache/6d237787c48fd1be900fe067cf1e0d81.json","text":"Intro to Database Systems 15-445/15-645 Fall 2019 Andy Pavlo Computer Science Carnegie Mellon UniversityAP 24 Distributed OLAP Databases CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Homework #5: Monday Dec 3rd @ 11:59pm Project #4: Monday Dec 10th @ 11:59pm Extra Credit: Wednesday Dec 10th @ 11:59pm Final Exam: Monday Dec 9th @ 5:30pm Systems Potpourri: Wednesday Dec 4th → Vote for what system you want me to talk about. → https://cmudb.io/f19-systems 2 CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Monday Dec 2nd – Oracle Lecture → Shasank Chavan (VP In-Memory Databases) Monday Dec 2nd – Oracle Systems Talk → 4:30pm in GHC 6115 → Pizza will be served Tuesday Dec 3rd – Oracle Research Talk → Hideaki Kimura (Oracle Beast) → 12:00pm in CIC 4th Floor (Panther Hollow Room) → Pizza will be served. 3 CMU 15-445/645 (Fall 2019) L A S T C L A S S Atomic Commit Protocols Replication Consistency Issues (CAP) Federated Databases 4 CMU 15-445/645 (Fall 2019) B I F U R C AT E D E N V I R O N M E N T 5 Extract Transform Load OLAP DatabaseOLTP Databases CMU 15-445/645 (Fall 2019) D E C I S I O N S U P P O R T S Y S T E M S Applications that serve the management, operations, and planning levels of an organization to help people make decisions about future issues and problems by analyzing historical data. Star Schema vs. Snowflake Schema 6 CMU 15-445/645 (Fall 2019) S TA R S C H E M A 7 CATEGORY_NAME CATEGORY_DESC PRODUCT_CODE PRODUCT_NAME PRODUCT_DESC PRODUCT_DIM COUNTRY STATE_CODE STATE_NAME ZIP_CODE CITY LOCATION_DIM ID FIRST_NAME LAST_NAME EMAIL ZIP_CODE CUSTOMER_DIM YEAR DAY_OF_YEAR MONTH_NUM MONTH_NAME DAY_OF_MONTH TIME_DIM SALES_FACT PRODUCT_FK TIME_FK LOCATION_FK CUSTOMER_FK PRICE QUANTITY CMU 15-445/645 (Fall 2019) S N O W F L A K E S C H E M A 8 CATEGORY_FK PRODUCT_CODE PRODUCT_NAME PRODUCT_DESC PRODUCT_DIM COUNTRY STATE_FK ZIP_CODE CITY LOCATION_DIM ID FIRST_NAME LAST_NAME EMAIL ZIP_CODE CUSTOMER_DIM YEAR DAY_OF_YEAR MONTH_FK DAY_OF_MONTH TIME_DIM SALES_FACT PRODUCT_FK TIME_FK LOCATION_FK CUSTOMER_FK PRICE QUANTITY CATEGORY_ID CATEGORY_NAME CATEGORY_DESC CAT_LOOKUP STATE_ID STATE_CODE STATE_NAME STATE_LOOKUP MONTH_NUM MONTH_NAME MONTH_SEASON MONTH_LOOKUP CMU 15-445/645 (Fall 2019) S TA R V S . S N O W F L A K E S C H E M A Issue #1: Normalization → Snowflake schemas take up less storage space. → Denormalized data models may incur integrity and consistency violations. Issue #2: Query Complexity → Snowflake schemas require more joins to get the data needed for a query. → Queries on star schemas will (usually) be faster. 9 CMU 15-445/645 (Fall 2019) P3 P4 P1 P2 P R O B L E M S E T U P 10 Application Server PartitionsSELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) P3 P4 P1 P2 P R O B L E M S E T U P 10 Application Server PartitionsSELECT * FROM R JOIN S ON R.id = S.id P2 P4 P3 CMU 15-445/645 (Fall 2019) T O D AY ' S A G E N D A Execution Models Query Planning Distributed Join Algorithms Cloud Systems 11 CMU 15-445/645 (Fall 2019) P U S H V S . P U L L Approach #1: Push Query to Data → Send the query (or a portion of it) to the node that contains the data. → Perform as much filtering and processing as possible where data resides before transmitting over network. Approach #2: Pull Data to Query → Bring the data to the node that is executing a query that needs it for processing. 12 CMU 15-445/645 (Fall 2019) P U S H Q U E R Y T O D ATA 13 Node Application Server Node P1→ID:1-100 P2→ID:101-200 SELECT * FROM R JOIN S ON R.id = S.id R ⨝ S IDs [101,200] Result: R ⨝ S CMU 15-445/645 (Fall 2019) Storage P U L L D ATA T O Q U E R Y 14 Node Application Server Node Page ABC Page XYZ R ⨝ S IDs [101,200] P1→ID:1-100 P2→ID:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) Storage P U L L D ATA T O Q U E R Y 14 Node Application Server Node Page ABC Page XYZ R ⨝ S IDs [101,200] P1→ID:1-100 P2→ID:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) Storage P U L L D ATA T O Q U E R Y 14 Node Application Server Node R ⨝ S IDs [101,200] P1→ID:1-100 P2→ID:101-200 SELECT * FROM R JOIN S ON R.id = S.id Result: R ⨝ S CMU 15-445/645 (Fall 2019) O B S E R VAT I O N The data that a node receives from remote sources are cached in the buffer pool. → This allows the DBMS to support intermediate results that are large than the amount of memory available. → Ephemeral pages are not persisted after a restart. What happens to a long-running OLAP query if a node crashes during execution? 15 CMU 15-445/645 (Fall 2019) Q U E R Y FA U LT T O L E R A N C E Most shared-nothing distributed OLAP DBMSs are designed to assume that nodes do not fail during query execution. → If one node fails during query execution, then the whole query fails. The DBMS could take a snapshot of the intermediate results for a query during execution to allow it to recover if nodes fail. 16 CMU 15-445/645 (Fall 2019) Storage Q U E R Y FA U LT T O L E R A N C E 17 Node Application Server Node R ⨝ S SELECT * FROM R JOIN S ON R.id = S.id Result: R ⨝ S CMU 15-445/645 (Fall 2019) Storage Q U E R Y FA U LT T O L E R A N C E 17 Node Application Server Node SELECT * FROM R JOIN S ON R.id = S.id Result: R ⨝ S CMU 15-445/645 (Fall 2019) Q U E R Y P L A N N I N G All the optimizations that we talked about before are still applicable in a distributed environment. → Predicate Pushdown → Early Projections → Optimal Join Orderings Distributed query optimization is even harder because it must consider the location of data in the cluster and data movement costs. 18 CMU 15-445/645 (Fall 2019) Q U E R Y P L A N F R A G M E N T S Approach #1: Physical Operators → Generate a single query plan and then break it up into partition-specific fragments. → Most systems implement this approach. Approach #2: SQL → Rewrite original query into partition-specific queries. → Allows for local optimization at each node. → MemSQL is the only system that I know that does this. 19 CMU 15-445/645 (Fall 2019) Q U E R Y P L A N F R A G M E N T S 20 SELECT * FROM R JOIN S ON R.id = S.id Id:1-100 SELECT * FROM R JOIN S ON R.id = S.id WHERE R.id BETWEEN 1 AND 100 Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id WHERE R.id BETWEEN 101 AND 200 Id:201-300 SELECT * FROM R JOIN S ON R.id = S.id WHERE R.id BETWEEN 201 AND 300 CMU 15-445/645 (Fall 2019) Q U E R Y P L A N F R A G M E N T S 20 SELECT * FROM R JOIN S ON R.id = S.id Id:1-100 SELECT * FROM R JOIN S ON R.id = S.id WHERE R.id BETWEEN 1 AND 100 Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id WHERE R.id BETWEEN 101 AND 200 Id:201-300 SELECT * FROM R JOIN S ON R.id = S.id WHERE R.id BETWEEN 201 AND 300 Union the output of each join to produce final result. CMU 15-445/645 (Fall 2019) O B S E R VAT I O N The efficiency of a distributed join depends on the target tables' partitioning schemes. One approach is to put entire tables on a single node and then perform the join. → You lose the parallelism of a distributed DBMS. → Costly data transfer over the network. 21 CMU 15-445/645 (Fall 2019) D I S T R I B U T E D J O I N A LG O R I T H M S To join tables R and S, the DBMS needs to get the proper tuples on the same node. Once there, it then executes the same join algorithms that we discussed earlier in the semester. 22 CMU 15-445/645 (Fall 2019) S C E N A R I O # 1 One table is replicated at every node. Each node joins its local data and then sends their results to a coordinating node. 23 R{Id} S Id:1-100 Replicated R{Id} S Id:101-200 Replicated SELECT * FROM R JOIN S ON R.id = S.id P1:R⨝S P2:R⨝S CMU 15-445/645 (Fall 2019) S C E N A R I O # 1 One table is replicated at every node. Each node joins its local data and then sends their results to a coordinating node. 23 R{Id} S Id:1-100 Replicated R{Id} S Id:101-200 Replicated SELECT * FROM R JOIN S ON R.id = S.id P1:R⨝S P2:R⨝S R⨝S CMU 15-445/645 (Fall 2019) S C E N A R I O # 2 Tables are partitioned on the join attribute. Each node performs the join on local data and then sends to a node for coalescing. 24 R{Id} S{Id} Id:1-100 R{Id} S{Id} Id:101-200 Id:1-100 Id:101-200 P1:R⨝S P2:R⨝S SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 2 Tables are partitioned on the join attribute. Each node performs the join on local data and then sends to a node for coalescing. 24 R{Id} S{Id} Id:1-100 R{Id} S{Id} Id:101-200 Id:1-100 Id:101-200 P1:R⨝S P2:R⨝S R⨝S SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 3 Both tables are partitioned on different keys. If one of the tables is small, then the DBMS broadcasts that table to all nodes. 25 R{Id} S{Val} Id:1-100 R{Id} S{Val} Id:101-200 Val:1-50 Val:51-100 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 3 Both tables are partitioned on different keys. If one of the tables is small, then the DBMS broadcasts that table to all nodes. 25 R{Id} S{Val} Id:1-100 R{Id} S{Val} Id:101-200 Val:1-50 Val:51-100 S SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 3 Both tables are partitioned on different keys. If one of the tables is small, then the DBMS broadcasts that table to all nodes. 25 R{Id} S{Val} Id:1-100 R{Id} S{Val} Id:101-200 Val:1-50 Val:51-100 S S SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 3 Both tables are partitioned on different keys. If one of the tables is small, then the DBMS broadcasts that table to all nodes. 25 R{Id} S{Val} Id:1-100 R{Id} S{Val} Id:101-200 Val:1-50 Val:51-100 S S P1:R⨝S P2:R⨝S SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 3 Both tables are partitioned on different keys. If one of the tables is small, then the DBMS broadcasts that table to all nodes. 25 R{Id} S{Val} Id:1-100 R{Id} S{Val} Id:101-200 Val:1-50 Val:51-100 S S P1:R⨝S P2:R⨝S R⨝S SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 R{Id} Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 R{Id}Id:1-100 R{Id} Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 Id:101-200S{Id} R{Id}Id:1-100 R{Id} Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 Id:1-100 S{Id} Id:101-200S{Id} R{Id}Id:1-100 R{Id} Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 Id:1-100 S{Id} Id:101-200S{Id} P1:R⨝S P2:R⨝S R{Id}Id:1-100 R{Id} Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S C E N A R I O # 4 Both tables are not partitioned on the join key. The DBMS copies the tables by reshuffling them across nodes. 26 R{Name} S{Val} Name:A-M R{Name} S{Val} Name:N-Z Val:1-50 Val:51-100 Id:1-100 S{Id} Id:101-200S{Id} P1:R⨝S P2:R⨝S R⨝S R{Id}Id:1-100 R{Id} Id:101-200 SELECT * FROM R JOIN S ON R.id = S.id CMU 15-445/645 (Fall 2019) S E M I - J O I N Join operator where the result only contains columns from the left table. Distributed DBMSs use semi-join to minimize the amount of data sent during joins. → This is like a projection pushdown. Some DBMSs support SEMI JOIN SQL syntax. Otherwise you fake it with EXISTS. 27 SELECT R.id FROM R LEFT OUTER JOIN S ON R.id = S.id WHERE R.id IS NOT NULL R S S CMU 15-445/645 (Fall 2019) S E M I - J O I N Join operator where the result only contains columns from the left table. Distributed DBMSs use semi-join to minimize the amount of data sent during joins. → This is like a projection pushdown. Some DBMSs support SEMI JOIN SQL syntax. Otherwise you fake it with EXISTS. 27 SELECT R.id FROM R LEFT OUTER JOIN S ON R.id = S.id WHERE R.id IS NOT NULL R S R CMU 15-445/645 (Fall 2019) S E M I - J O I N Join operator where the result only contains columns from the left table. Distributed DBMSs use semi-join to minimize the amount of data sent during joins. → This is like a projection pushdown. Some DBMSs support SEMI JOIN SQL syntax. Otherwise you fake it with EXISTS. 27 SELECT R.id FROM R LEFT OUTER JOIN S ON R.id = S.id WHERE R.id IS NOT NULL R S SELECT R.id FROM R WHERE EXISTS ( SELECT 1 FROM S WHERE R.id = S.id) R.id R.id CMU 15-445/645 (Fall 2019) S E M I - J O I N Join operator where the result only contains columns from the left table. Distributed DBMSs use semi-join to minimize the amount of data sent during joins. → This is like a projection pushdown. Some DBMSs support SEMI JOIN SQL syntax. Otherwise you fake it with EXISTS. 27 SELECT R.id FROM R LEFT OUTER JOIN S ON R.id = S.id WHERE R.id IS NOT NULL R S SELECT R.id FROM R WHERE EXISTS ( SELECT 1 FROM S WHERE R.id = S.id) R.id R.id CMU 15-445/645 (Fall 2019) R E L AT I O N A L A LG E B R A : S E M I - J O I N Like a natural join except that the attributes on the right table that are not used to compute the join are restricted. Syntax: (R⋉ S) 28 a_id b_id xxx a1 101 X1 a2 102 X2 a3 103 X3 R(a_id,b_id,xxx) S(a_id,b_id,yyy) a_id b_id yyy a3 103 Y1 a4 104 Y2 a5 105 Y3 (R ⋉ S) a_id b_id xxx a3 103 X3 CMU 15-445/645 (Fall 2019) C LO U D S Y S T E M S Vendors provide database-as-a-service (DBaaS) offerings that are managed DBMS environments. Newer systems are starting to blur the lines between shared-nothing and shared-disk. → Example: You can do simple filtering on Amazon S3 before copying data to compute nodes. 29 CMU 15-445/645 (Fall 2019) C LO U D S Y S T E M S Approach #1: Managed DBMSs → No significant modification to the DBMS to be \"aware\" that it is running in a cloud environment. → Examples: Most vendors Approach #2: Cloud-Native DBMS → The system is designed explicitly to run in a cloud environment. → Usually based on a shared-disk architecture. → Examples: Snowflake, Google BigQuery, Amazon Redshift, Microsoft SQL Azure 30 CMU 15-445/645 (Fall 2019) S E R V E R L E S S D ATA B A S E S Rather than always maintaining compute resources for each customer, a \"serverless\" DBMS evicts tenants when they become idle. 31 Application Server Node CMU 15-445/645 (Fall 2019) S E R V E R L E S S D ATA B A S E S Rather than always maintaining compute resources for each customer, a \"serverless\" DBMS evicts tenants when they become idle. 31 Application Server Node CMU 15-445/645 (Fall 2019) S E R V E R L E S S D ATA B A S E S Rather than always maintaining compute resources for each customer, a \"serverless\" DBMS evicts tenants when they become idle. 31 Application Server Node Storage CMU 15-445/645 (Fall 2019) S E R V E R L E S S D ATA B A S E S Rather than always maintaining compute resources for each customer, a \"serverless\" DBMS evicts tenants when they become idle. 31 Application Server Node Storage Buffer Pool Page Table CMU 15-445/645 (Fall 2019) S E R V E R L E S S D ATA B A S E S Rather than always maintaining compute resources for each customer, a \"serverless\" DBMS evicts tenants when they become idle. 31 Application Server Storage CMU 15-445/645 (Fall 2019) S E R V E R L E S S D ATA B A S E S Rather than always maintaining compute resources for each customer, a \"serverless\" DBMS evicts tenants when they become idle. 31 Application Server Node Storage Buffer Pool Page Table CMU 15-445/645 (Fall 2019) D I S A G G R E G AT E D C O M P O N E N T S System Catalogs → HCatalog, Google Data Catalog, Amazon Glue Data Catalog Node Management → Kubernetes, Apache YARN, Cloud Vendor Tools Query Optimizers → Greenplum Orca, Apache Calcite 32 CMU 15-445/645 (Fall 2019) U N I V E R S A L F O R M AT S Most DBMSs use a proprietary on-disk binary file format for their databases. → Think of the BusTub page types… The only way to share data between systems is to convert data into a common text-based format → Examples: CSV, JSON, XML There are new open-source binary file formats that make it easier to access data across systems. 33 CMU 15-445/645 (Fall 2019) U N I V E R S A L F O R M AT S Apache Parquet → Compressed columnar storage from Cloudera/Twitter Apache ORC → Compressed columnar storage from Apache Hive. Apache CarbonData → Compressed columnar storage with indexes from Huawei. 34 Apache Iceberg → Flexible data format that supports schema evolution from Netflix. HDF5 → Multi-dimensional arrays for scientific workloads. Apache Arrow → In-memory compressed columnar storage from Pandas/Dremio. CMU 15-445/645 (Fall 2019) C O N C L U S I O N More money, more data, more problems… Cloud OLAP Vendors: On-Premise OLAP Systems: 35 CMU 15-445/645 (Fall 2019) N E X T C L A S S Oracle Guest Speaker 36","libVersion":"0.2.2","langs":""}