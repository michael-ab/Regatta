{"path":".obsidian/plugins/text-extractor/cache/1c66a7fae14ab2bd5c2c8c8f49e2a65e.json","text":"Intro to Database Systems 15-445/15-645 Fall 2019 Andy Pavlo Computer Science Carnegie Mellon UniversityAP 22 Introduction to Distributed Databases CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Homework #5: Monday Dec 3rd @ 11:59pm Project #4: Monday Dec 10th @ 11:59pm Extra Credit: Wednesday Dec 10th @ 11:59pm Final Exam: Monday Dec 9th @ 5:30pm 2 CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Monday Dec 2th – Oracle Lecture → Shasank Chavan (VP In-Memory Databases) Wednesday Dec 4th – Potpourri + Review → Vote for what system you want me to talk about. → https://cmudb.io/f19-systems Sunday Nov 24th – Extra Credit Check → Submit your extra credit assignment early to get feedback from me. 3 CMU 15-445/645 (Fall 2019) U P C O M I N G D ATA B A S E E V E N T S Oracle Research Talk → Tuesday December 4th @ 12:00pm → CIC 4th Floor 4 CMU 15-445/645 (Fall 2019) PA R A L L E L V S . D I S T R I B U T E D Parallel DBMSs: → Nodes are physically close to each other. → Nodes connected with high-speed LAN. → Communication cost is assumed to be small. Distributed DBMSs: → Nodes can be far from each other. → Nodes connected using public network. → Communication cost and problems cannot be ignored. 5 CMU 15-445/645 (Fall 2019) D I S T R I B U T E D D B M S s Use the building blocks that we covered in single- node DBMSs to now support transaction processing and query execution in distributed environments. → Optimization & Planning → Concurrency Control → Logging & Recovery 6 CMU 15-445/645 (Fall 2019) T O D AY ' S A G E N D A System Architectures Design Issues Partitioning Schemes Distributed Concurrency Control 7 CMU 15-445/645 (Fall 2019) S Y S T E M A R C H I T E C T U R E A DBMS's system architecture specifies what shared resources are directly accessible to CPUs. This affects how CPUs coordinate with each other and where they retrieve/store objects in the database. 8 CMU 15-445/645 (Fall 2019) S Y S T E M A R C H I T E C T U R E 9 Shared Nothing Network Shared Memory Network Shared Disk Network Shared Everything CMU 15-445/645 (Fall 2019) S H A R E D M E M O R Y CPUs have access to common memory address space via a fast interconnect. → Each processor has a global view of all the in-memory data structures. → Each DBMS instance on a processor has to \"know\" about the other instances. 10 Network CMU 15-445/645 (Fall 2019) S H A R E D D I S K All CPUs can access a single logical disk directly via an interconnect, but each have their own private memories. → Can scale execution layer independently from the storage layer. → Must send messages between CPUs to learn about their current state. 11 Network CMU 15-445/645 (Fall 2019) Storage S H A R E D D I S K E X A M P L E 12 Node Application Server Node Node Update 101 Get Id=200 Get Id=101 Page ABC Page XYZ Get Id=101 Page ABC CMU 15-445/645 (Fall 2019) S H A R E D N O T H I N G Each DBMS instance has its own CPU, memory, and disk. Nodes only communicate with each other via network. → Hard to increase capacity. → Hard to ensure consistency. → Better performance & efficiency. 13 Network CMU 15-445/645 (Fall 2019) S H A R E D N O T H I N G E X A M P L E 14 Node Application Server Node P1→ID:1-150 P2→ID:151-300 Node P3→ID:101-200 P1→ID:1-100 P2→ID:201-300 Get Id=200 Get Id=10 Get Id=200 Get Id=200 CMU 15-445/645 (Fall 2019) E A R LY D I S T R I B U T E D D ATA B A S E S Y S T E M S MUFFIN – UC Berkeley (1979) SDD-1 – CCA (1979) System R* – IBM Research (1984) Gamma – Univ. of Wisconsin (1986) NonStop SQL – Tandem (1987) 15 Bernstein Mohan DeWitt Gray Stonebraker CMU 15-445/645 (Fall 2019) D E S I G N I S S U E S How does the application find data? How to execute queries on distributed data? → Push query to data. → Pull data to query. How does the DBMS ensure correctness? 16 CMU 15-445/645 (Fall 2019) H O M O G E N O U S V S . H E T E R O G E N O U S Approach #1: Homogenous Nodes → Every node in the cluster can perform the same set of tasks (albeit on potentially different partitions of data). → Makes provisioning and failover \"easier\". Approach #2: Heterogenous Nodes → Nodes are assigned specific tasks. → Can allow a single physical node to host multiple \"virtual\" node types for dedicated tasks. 17 CMU 15-445/645 (Fall 2019) M O N G O D B H E T E R O G E N O U S A R C H I T E C T U R E 18 Router (mongos) Shards (mongod) P3 P4 P1 P2 P1→ID:1-100 P2→ID:101-200 P3→ID:201-300 P4→ID:301-400 Config Server (mongod) Router (mongos) ⋮ ⋮ Application Server Get Id=101 CMU 15-445/645 (Fall 2019) D ATA T R A N S PA R E N C Y Users should not be required to know where data is physically located, how tables are partitioned or replicated. A SQL query that works on a single-node DBMS should work the same on a distributed DBMS. 19 CMU 15-445/645 (Fall 2019) D ATA B A S E PA R T I T I O N I N G Split database across multiple resources: → Disks, nodes, processors. → Sometimes called \"sharding\" The DBMS executes query fragments on each partition and then combines the results to produce a single answer. 20 CMU 15-445/645 (Fall 2019) N A Ï V E TA B L E PA R T I T I O N I N G Each node stores one and only table. Assumes that each node has enough storage space for a table. 21 CMU 15-445/645 (Fall 2019) N A Ï V E TA B L E PA R T I T I O N I N G 22 Table1 SELECT * FROM table Ideal Query: Table2 Partitions Table1 Table2 CMU 15-445/645 (Fall 2019) H O R I Z O N TA L PA R T I T I O N I N G Split a table's tuples into disjoint subsets. → Choose column(s) that divides the database equally in terms of size, load, or usage. → Hash Partitioning, Range Partitioning The DBMS can partition a database physical (shared nothing) or logically (shared disk). 23 CMU 15-445/645 (Fall 2019) H O R I Z O N TA L PA R T I T I O N I N G 24 SELECT * FROM table WHERE partitionKey = ? Ideal Query: PartitionsTable1 101 a XXX 2019-11-29 102 b XXY 2019-11-28 103 c XYZ 2019-11-29 104 d XYX 2019-11-27 105 e XYY 2019-11-29 P3 P4 P1 P2 hash(a)%4 = P2 hash(b)%4 = P4 hash(c)%4 = P3 hash(d)%4 = P2 hash(e)%4 = P1 Partitioning Key CMU 15-445/645 (Fall 2019) C O N S I S T E N T H A S H I N G 25 01 1/2 Replication Factor = 3 hash(key2) hash(key1) If hash(key)=D E A C D B F CMU 15-445/645 (Fall 2019) Storage LO G I C A L PA R T I T I O N I N G 26 Node Application Server Node Get Id=1 Get Id=3 Id=1 Id=2 Id=3 Id=4 Id=1 Id=2 Id=3 Id=4 CMU 15-445/645 (Fall 2019) Node Node P H Y S I C A L PA R T I T I O N I N G 27 Application Server Get Id=1 Get Id=3 Id=1 Id=2 Id=3 Id=4 CMU 15-445/645 (Fall 2019) S I N G L E- N O D E V S . D I S T R I B U T E D A single-node txn only accesses data that is contained on one partition. → The DBMS does not need coordinate the behavior concurrent txns running on other nodes. A distributed txn accesses data at one or more partitions. → Requires expensive coordination. 29 CMU 15-445/645 (Fall 2019) T R A N S A C T I O N C O O R D I N AT I O N If our DBMS supports multi-operation and distributed txns, we need a way to coordinate their execution in the system. Two different approaches: → Centralized: Global \"traffic cop\". → Decentralized: Nodes organize themselves. 30 CMU 15-445/645 (Fall 2019) T P M O N I T O R S Example of a centralized coordinator. Originally developed in the 1970-80s to provide txns between terminals and mainframe databases. → Examples: ATMs, Airline Reservations. Many DBMSs now support the same functionality internally. 31 CMU 15-445/645 (Fall 2019) Coordinator C E N T R A L I Z E D C O O R D I N AT O R 32 PartitionsLock Request Acknowledgement Commit Request Safe to commit?Application Server P3 P4 P1 P2 P1 P2 P3 P4 CMU 15-445/645 (Fall 2019) C E N T R A L I Z E D C O O R D I N AT O R 33Middleware Query Requests Safe to commit? Application Server P3 P4 P1 P2 P1→ID:1-100 P2→ID:101-200 P3→ID:201-300 P4→ID:301-400 Commit Request Partitions CMU 15-445/645 (Fall 2019) P3 P4 P1 P2 D E C E N T R A L I Z E D C O O R D I N AT O R 34 Application Server Safe to commit? Begin Request Query Request Commit Request Partitions CMU 15-445/645 (Fall 2019) D I S T R I B U T E D C O N C U R R E N C Y C O N T R O L Need to allow multiple txns to execute simultaneously across multiple nodes. → Many of the same protocols from single-node DBMSs can be adapted. This is harder because of: → Replication. → Network Communication Overhead. → Node Failures. → Clock Skew. 35 CMU 15-445/645 (Fall 2019) D I S T R I B U T E D 2 P L 36 Node 1 Node 2 NETWORK Set A=2 A=1A=2 Set B=7 B=8B=7 Application Server Application ServerSet B=9 Set A=0 Waits-For Graph T1 T2 CMU 15-445/645 (Fall 2019) C O N C L U S I O N I have barely scratched the surface on distributed database systems… It is hard to get right. More info (and humiliation): → Kyle Kingsbury's Jepsen Project 37 CMU 15-445/645 (Fall 2019) N E X T C L A S S Distributed OLTP Systems Replication CAP Theorem Real-World Examples 38","libVersion":"0.2.2","langs":""}