{"path":".obsidian/plugins/text-extractor/cache/f9846cb008fb86247f54a0ea63e0872d.json","text":"Intro to Database Systems 15-445/15-645 Fall 2019 Andy Pavlo Computer Science Carnegie Mellon UniversityAP 23 Distributed OLTP Databases CMU 15-445/645 (Fall 2019) A D M I N I S T R I V I A Homework #5: Monday Dec 3rd @ 11:59pm Project #4: Monday Dec 10th @ 11:59pm Extra Credit: Wednesday Dec 10th @ 11:59pm Final Exam: Monday Dec 9th @ 5:30pm 2 CMU 15-445/645 (Fall 2019) L A S T C L A S S System Architectures → Shared-Memory, Shared-Disk, Shared-Nothing Partitioning/Sharding → Hash, Range, Round Robin Transaction Coordination → Centralized vs. Decentralized 3 CMU 15-445/645 (Fall 2019) O LT P V S . O L A P On-line Transaction Processing (OLTP): → Short-lived read/write txns. → Small footprint. → Repetitive operations. On-line Analytical Processing (OLAP): → Long-running, read-only queries. → Complex joins. → Exploratory queries. 4 CMU 15-445/645 (Fall 2019) P3 P4 P1 P2 D E C E N T R A L I Z E D C O O R D I N AT O R 5 Application Server Begin Request Partitions CMU 15-445/645 (Fall 2019) P3 P4 P1 P2 D E C E N T R A L I Z E D C O O R D I N AT O R 5 Application Server Query Partitions Query Query CMU 15-445/645 (Fall 2019) P3 P4 P1 P2 D E C E N T R A L I Z E D C O O R D I N AT O R 5 Application Server Safe to commit? Commit Request Partitions CMU 15-445/645 (Fall 2019) O B S E RVAT I O N We have not discussed how to ensure that all nodes agree to commit a txn and then to make sure it does commit if we decide that it should. → What happens if a node fails? → What happens if our messages show up late? → What happens if we don't wait for every node to agree? 6 CMU 15-445/645 (Fall 2019) I M P O R TA N T A S S U M P T I O N We can assume that all nodes in a distributed DBMS are well-behaved and under the same administrative domain. → If we tell a node to commit a txn, then it will commit the txn (if there is not a failure). If you do not trust the other nodes in a distributed DBMS, then you need to use a Byzantine Fault Tolerant protocol for txns (blockchain). 7 CMU 15-445/645 (Fall 2019) TO D AY ' S A G E N D A Atomic Commit Protocols Replication Consistency Issues (CAP) Federated Databases 8 CMU 15-445/645 (Fall 2019) ATO M I C C O M M I T P R OTO C O L When a multi-node txn finishes, the DBMS needs to ask all the nodes involved whether it is safe to commit. Examples: → Two-Phase Commit → Three-Phase Commit (not used) → Paxos → Raft → ZAB (Apache Zookeeper) → Viewstamped Replication 9 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( S U C C E S S ) 10 Commit RequestParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( S U C C E S S ) 10 Commit Request Phase1: PrepareParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( S U C C E S S ) 10 Commit Request OK OK Phase1: PrepareParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( S U C C E S S ) 10 Commit Request OK OK Phase1: PrepareParticipantParticipantCoordinator Application Server Node 3 Node 2 Phase2: Commit CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( S U C C E S S ) 10 Commit Request OK OK OK Phase1: PrepareParticipantParticipantCoordinator Application Server Node 3 Node 2 Phase2: Commit OK CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( S U C C E S S ) 10ParticipantParticipantCoordinator Application Server Node 3 Node 2 Success! CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( A B O R T ) 11 Commit RequestParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( A B O R T ) 11 Commit Request Phase1: PrepareParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( A B O R T ) 11 Commit Request ABORT! Phase1: PrepareParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( A B O R T ) 11 ABORT!ParticipantParticipantCoordinator Application Server Node 3 Node 2 Aborted CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( A B O R T ) 11 ABORT!ParticipantParticipantCoordinator Application Server Node 3 Node 2 Phase2: Abort Aborted CMU 15-445/645 (Fall 2019) Node 1 T W O -P H A S E C O M M I T ( A B O R T ) 11 ABORT! OKParticipantParticipantCoordinator Application Server Node 3 Node 2 Phase2: Abort OK Aborted CMU 15-445/645 (Fall 2019) 2 P C O P T I M I Z AT I O N S Early Prepare Voting → If you send a query to a remote node that you know will be the last one you execute there, then that node will also return their vote for the prepare phase with the query result. Early Acknowledgement After Prepare → If all nodes vote to commit a txn, the coordinator can send the client an acknowledgement that their txn was successful before the commit phase finishes. 12 CMU 15-445/645 (Fall 2019) Node 1 E A R LY A C K N O W L E D G E M E N T 13 Commit RequestParticipantParticipantCoordinator Application Server Node 3 Node 2 CMU 15-445/645 (Fall 2019) Node 1 E A R LY A C K N O W L E D G E M E N T 13 Commit RequestParticipantParticipantCoordinator Application Server Node 3 Node 2Phase1: Prepare CMU 15-445/645 (Fall 2019) Node 1 E A R LY A C K N O W L E D G E M E N T 13 Commit Request OK OKParticipantParticipantCoordinator Application Server Node 3 Node 2Phase1: Prepare CMU 15-445/645 (Fall 2019) Node 1 E A R LY A C K N O W L E D G E M E N T 13 OK OKParticipantParticipantCoordinator Application Server Node 3 Node 2 Success! Phase1: Prepare CMU 15-445/645 (Fall 2019) Node 1 E A R LY A C K N O W L E D G E M E N T 13 OK OKParticipantParticipantCoordinator Application Server Node 3 Node 2 Success! Phase1: Prepare Phase2: Commit CMU 15-445/645 (Fall 2019) Node 1 E A R LY A C K N O W L E D G E M E N T 13 OK OK OKParticipantParticipantCoordinator Application Server Node 3 Node 2 OK Success! Phase1: Prepare Phase2: Commit CMU 15-445/645 (Fall 2019) T W O -P H A S E C O M M I T Each node records the outcome of each phase in a non-volatile storage log. What happens if coordinator crashes? → Participants must decide what to do. What happens if participant crashes? → Coordinator assumes that it responded with an abort if it hasn't sent an acknowledgement yet. 14 CMU 15-445/645 (Fall 2019) PA XO S Consensus protocol where a coordinator proposes an outcome (e.g., commit or abort) and then the participants vote on whether that outcome should succeed. Does not block if a majority of participants are available and has provably minimal message delays in the best case. 15 CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16 Commit RequestAcceptorAcceptorProposer Application Server Node 4 Node 2AcceptorNode 3 CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16 Commit RequestAcceptorAcceptorProposer Application Server Node 4 Node 2AcceptorNode 3 Propose CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16 Commit RequestAcceptorAcceptorProposer Application Server Node 4 Node 2AcceptorNode 3XPropose CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16 Commit Request Agree AgreeAcceptorAcceptorProposer Application Server Node 4 Node 2AcceptorNode 3XPropose CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16 Commit Request Agree AgreeAcceptorAcceptorProposer Application Server Node 4 Node 2AcceptorNode 3XPropose Commit CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16 Commit Request Agree Agree AcceptAcceptorAcceptorProposer Application Server Node 4 Node 2 AcceptAcceptorNode 3XPropose Commit CMU 15-445/645 (Fall 2019) Node 1 PA XO S 16AcceptorAcceptorProposer Application Server Node 4 Node 2 Success!AcceptorNode 3X CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptorsTIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n) Propose(n+1)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n) Propose(n+1) Commit(n)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n) Propose(n+1) Commit(n) Reject(n,n+1)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n) Propose(n+1) Commit(n) Reject(n,n+1) Agree(n+1)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n) Propose(n+1) Commit(n) Reject(n,n+1) Commit(n+1) Agree(n+1)TIME CMU 15-445/645 (Fall 2019) PA XO S 17 Proposer ProposerAcceptors Propose(n) Agree(n) Propose(n+1) Commit(n) Reject(n,n+1) Commit(n+1) Agree(n+1) Accept(n+1)TIME CMU 15-445/645 (Fall 2019) M U LT I -PA X O S If the system elects a single leader that is in charge of proposing changes for some period of time, then it can skip the Propose phase. → Fall back to full Paxos whenever there is a failure. The system periodically renews who the leader is using another Paxos round. 18 CMU 15-445/645 (Fall 2019) 2 P C V S . PA XO S Two-Phase Commit → Blocks if coordinator fails after the prepare message is sent, until coordinator recovers. Paxos → Non-blocking if a majority participants are alive, provided there is a sufficiently long period without further failures. 19 CMU 15-445/645 (Fall 2019) R E P L I C AT I O N The DBMS can replicate data across redundant nodes to increase availability. Design Decisions: → Replica Configuration → Propagation Scheme → Propagation Timing → Update Method 20 CMU 15-445/645 (Fall 2019) R E P L I C A C O N F I G U R AT I O N S Approach #1: Master-Replica → All updates go to a designated master for each object. → The master propagates updates to its replicas without an atomic commit protocol. → Read-only txns may be allowed to access replicas. → If the master goes down, then hold an election to select a new master. Approach #2: Multi-Master → Txns can update data objects at any replica. → Replicas must synchronize with each other using an atomic commit protocol. 21 CMU 15-445/645 (Fall 2019) R E P L I C A C O N F I G U R AT I O N S 22 Master-Replica Master P1 P1 P1 Replicas Multi-Master Node 1 P1 Node 2 P1 Writes Reads Writes Reads Writes Reads Reads CMU 15-445/645 (Fall 2019) K-S A F E T Y K-safety is a threshold for determining the fault tolerance of the replicated database. The value K represents the number of replicas per data object that must always be available. If the number of replicas goes below this threshold, then the DBMS halts execution and takes itself offline. 23 CMU 15-445/645 (Fall 2019) P R O PA G AT I O N S C H E M E When a txn commits on a replicated database, the DBMS decides whether it must wait for that txn's changes to propagate to other nodes before it can send the acknowledgement to application. Propagation levels: → Synchronous (Strong Consistency) → Asynchronous (Eventual Consistency) 24 CMU 15-445/645 (Fall 2019) P R O PA G AT I O N S C H E M E Approach #1: Synchronous → The master sends updates to replicas and then waits for them to acknowledge that they fully applied (i.e., logged) the changes. 25 Commit? Flush? Flush! CMU 15-445/645 (Fall 2019) P R O PA G AT I O N S C H E M E Approach #1: Synchronous → The master sends updates to replicas and then waits for them to acknowledge that they fully applied (i.e., logged) the changes. 25 Commit? Flush? AckAck Flush! CMU 15-445/645 (Fall 2019) P R O PA G AT I O N S C H E M E Approach #1: Synchronous → The master sends updates to replicas and then waits for them to acknowledge that they fully applied (i.e., logged) the changes. Approach #2: Asynchronous → The master immediately returns the acknowledgement to the client without waiting for replicas to apply the changes. 25 Commit? Flush? AckAck Flush! Commit? Flush? Ack CMU 15-445/645 (Fall 2019) P R O PA G AT I O N T I M I N G Approach #1: Continuous → The DBMS sends log messages immediately as it generates them. → Also need to send a commit/abort message. Approach #2: On Commit → The DBMS only sends the log messages for a txn to the replicas once the txn is commits. → Do not waste time sending log records for aborted txns. → Assumes that a txn's log records fits entirely in memory. 27 CMU 15-445/645 (Fall 2019) A C T I V E V S . PA S S I V E Approach #1: Active-Active → A txn executes at each replica independently. → Need to check at the end whether the txn ends up with the same result at each replica. Approach #2: Active-Passive → Each txn executes at a single location and propagates the changes to the replica. → Can either do physical or logical replication. → Not the same as master-replica vs. multi-master 28 CMU 15-445/645 (Fall 2019) C A P T H E O R E M Proposed by Eric Brewer that it is impossible for a distributed system to always be: → Consistent → Always Available → Network Partition Tolerant Proved in 2002. 29 Brewer Pick Two! Sort of… CMU 15-445/645 (Fall 2019) C A P T H E O R E M 30 AC P Consistency Availability Partition Tolerant Linearizability All up nodes can satisfy all requests. Still operate correctly despite message loss. Impossible CMU 15-445/645 (Fall 2019) C A P C O N S I S T E N C Y 31 Master Replica NETWORK Set A=2 A=1 B=8 A=1 B=8 Application Server Application Server CMU 15-445/645 (Fall 2019) C A P C O N S I S T E N C Y 31 Master Replica NETWORK Set A=2 A=1 B=8 A=2 A=1 B=8 Application Server Application Server CMU 15-445/645 (Fall 2019) C A P C O N S I S T E N C Y 31 Master Replica NETWORK Set A=2 A=1 B=8 A=2 A=1 B=8 A=2 Application Server Application Server CMU 15-445/645 (Fall 2019) C A P C O N S I S T E N C Y 31 Master Replica NETWORK Set A=2 A=1 B=8 A=2 A=1 B=8 A=2 Application Server Application ServerACK CMU 15-445/645 (Fall 2019) C A P C O N S I S T E N C Y 31 Master Replica NETWORK Set A=2 A=1 B=8 A=2 Read A A=1 B=8 A=2 Application Server Application ServerACK CMU 15-445/645 (Fall 2019) C A P C O N S I S T E N C Y 31 Master Replica NETWORK Set A=2 A=1 B=8 A=2 Read A A=2 A=1 B=8 A=2 If master says the txn committed, then it should be immediately visible on replicas. Application Server Application ServerACK CMU 15-445/645 (Fall 2019) C A P AVA I L A B I L I T Y 32 Master Replica NETWORK A=1 B=8 A=1 B=8 Application Server Application Server X CMU 15-445/645 (Fall 2019) C A P AVA I L A B I L I T Y 32 Master Replica NETWORK A=1 B=8 A=1 B=8 Application Server Application Server X Read B CMU 15-445/645 (Fall 2019) C A P AVA I L A B I L I T Y 32 Master Replica NETWORK A=1 B=8 A=1 B=8 Application Server Application Server X Read B B=8 CMU 15-445/645 (Fall 2019) C A P AVA I L A B I L I T Y 32 Master Replica NETWORK A=1 B=8 A=1 B=8 Application Server Application Server X Read A CMU 15-445/645 (Fall 2019) C A P AVA I L A B I L I T Y 32 Master Replica NETWORK A=1 B=8 A=1 B=8 Application Server Application Server X Read A A=1 CMU 15-445/645 (Fall 2019) C A P PA R T I T I O N TO L E R A N C E 33 Master Replica NETWORK A=1 B=8 A=1 B=8 Application Server Application Server CMU 15-445/645 (Fall 2019) C A P PA R T I T I O N TO L E R A N C E 33 Master A=1 B=8 A=1 B=8 Application Server Application Server Master CMU 15-445/645 (Fall 2019) C A P PA R T I T I O N TO L E R A N C E 33 Master Set A=2 A=1 B=8 Set A=3 A=1 B=8 Application Server Application Server Master CMU 15-445/645 (Fall 2019) C A P PA R T I T I O N TO L E R A N C E 33 Master Set A=2 A=1 B=8 A=2 Set A=3 A=1 B=8 A=3 Application Server Application Server Master CMU 15-445/645 (Fall 2019) C A P PA R T I T I O N TO L E R A N C E 33 Master Set A=2 A=1 B=8 A=2 Set A=3 ACK A=1 B=8 A=3 Application Server Application ServerACK Master CMU 15-445/645 (Fall 2019) C A P PA R T I T I O N TO L E R A N C E 33 Master NETWORK Set A=2 A=1 B=8 A=2 Set A=3 ACK A=1 B=8 A=3 Application Server Application ServerACK Master CMU 15-445/645 (Fall 2019) C A P F O R O LT P D B M S s How a DBMS handles failures determines which elements of the CAP theorem they support. Traditional/NewSQL DBMSs → Stop allowing updates until a majority of nodes are reconnected. NoSQL DBMSs → Provide mechanisms to resolve conflicts after nodes are reconnected. 34 CMU 15-445/645 (Fall 2019) O B S E RVAT I O N We have assumed that the nodes in our distributed systems are running the same DBMS software. But organizations often run many different DBMSs in their applications. It would be nice if we could have a single interface for all our data. 35 CMU 15-445/645 (Fall 2019) F E D E R AT E D D ATA B A S E S Distributed architecture that connects together multiple DBMSs into a single logical system. A query can access data at any location. This is hard and nobody does it well → Different data models, query languages, limitations. → No easy way to optimize queries → Lots of data copying (bad). 36 CMU 15-445/645 (Fall 2019) F E D E R AT E D D ATA B A S E E X A M P L E 37Middleware Query Requests Application Server Back-end DBMSs Connectors CMU 15-445/645 (Fall 2019) F E D E R AT E D D ATA B A S E E X A M P L E 37 Query Requests Application Server Back-end DBMSs Foreign Data Wrappers Connectors CMU 15-445/645 (Fall 2019) C O N C LU S I O N We assumed that the nodes in our distributed DBMS are friendly. Blockchain databases assume that the nodes are adversarial. This means you must use different protocols to commit transactions. 38 CMU 15-445/645 (Fall 2019) N E X T C L A S S Distributed OLAP Systems 39","libVersion":"0.2.2","langs":""}